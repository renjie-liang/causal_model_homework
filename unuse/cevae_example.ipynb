{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CEVAE vs. Meta-Learners Benchmark with IHDP + Synthetic Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T21:16:06.716322Z",
     "start_time": "2021-02-01T21:16:00.790891Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from scipy.stats import entropy\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "from causalml.inference.meta import BaseXRegressor, BaseRRegressor, BaseSRegressor, BaseTRegressor\n",
    "from causalml.inference.torch import CEVAE\n",
    "from causalml.propensity import ElasticNetPropensityModel\n",
    "from causalml.metrics import *\n",
    "from causalml.dataset import simulate_hidden_confounder\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logger = logging.getLogger('causalml')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_palette('Paired')\n",
    "plt.rcParams['figure.figsize'] = (12,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IHDP semi-synthetic dataset\n",
    "\n",
    "Hill introduced a semi-synthetic dataset constructed from the Infant Health\n",
    "and Development Program (IHDP). This dataset is based on a randomized experiment\n",
    "investigating the effect of home visits by specialists on future cognitive scores. The IHDP simulation is considered the de-facto standard benchmark for neural network treatment effect\n",
    "estimation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T21:16:07.130301Z",
     "start_time": "2021-02-01T21:16:06.722641Z"
    }
   },
   "outputs": [],
   "source": [
    "# load all ihadp data\n",
    "df = pd.DataFrame()\n",
    "for i in range(1, 10):\n",
    "    data = pd.read_csv('./data/ihdp_npci_' + str(i) + '.csv', header=None)\n",
    "    df = pd.concat([data, df])\n",
    "cols =  [\"treatment\", \"y_factual\", \"y_cfactual\", \"mu0\", \"mu1\"] + [i for i in range(25)]\n",
    "df.columns = cols\n",
    "print(df.shape)\n",
    "\n",
    "# replicate the data 100 times\n",
    "replications = 100\n",
    "df = pd.concat([df]*replications, ignore_index=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T21:16:07.144511Z",
     "start_time": "2021-02-01T21:16:07.139182Z"
    }
   },
   "outputs": [],
   "source": [
    "# set which features are binary\n",
    "binfeats = [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
    "# set which features are continuous\n",
    "contfeats = [i for i in range(25) if i not in binfeats]\n",
    "\n",
    "# reorder features with binary first and continuous after\n",
    "perm = binfeats + contfeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T21:16:07.366309Z",
     "start_time": "2021-02-01T21:16:07.152398Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T21:16:34.604702Z",
     "start_time": "2021-02-01T21:16:07.370970Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df[perm].values\n",
    "treatment = df['treatment'].values\n",
    "y = df['y_factual'].values\n",
    "y_cf = df['y_cfactual'].values\n",
    "tau = df.apply(lambda d: d['y_factual'] - d['y_cfactual'] if d['treatment']==1\n",
    "               else d['y_cfactual'] - d['y_factual'],\n",
    "               axis=1)\n",
    "mu_0 = df['mu0'].values\n",
    "mu_1 = df['mu1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T21:16:35.018237Z",
     "start_time": "2021-02-01T21:16:34.606960Z"
    }
   },
   "outputs": [],
   "source": [
    "# seperate for train and test\n",
    "itr, ite = train_test_split(np.arange(X.shape[0]), test_size=0.2, random_state=1)\n",
    "X_train, treatment_train, y_train, y_cf_train, tau_train, mu_0_train, mu_1_train = X[itr], treatment[itr], y[itr], y_cf[itr], tau[itr], mu_0[itr], mu_1[itr]\n",
    "X_val, treatment_val, y_val, y_cf_val, tau_val, mu_0_val, mu_1_val = X[ite], treatment[ite], y[ite], y_cf[ite], tau[ite], mu_0[ite], mu_1[ite]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CEVAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T21:16:35.024352Z",
     "start_time": "2021-02-01T21:16:35.020848Z"
    }
   },
   "outputs": [],
   "source": [
    "# cevae model settings\n",
    "outcome_dist = \"normal\"\n",
    "latent_dim = 20\n",
    "hidden_dim = 200\n",
    "num_epochs = 5\n",
    "batch_size = 1000\n",
    "learning_rate = 0.001\n",
    "learning_rate_decay = 0.01\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T21:16:35.032884Z",
     "start_time": "2021-02-01T21:16:35.029438Z"
    }
   },
   "outputs": [],
   "source": [
    "cevae = CEVAE(outcome_dist=outcome_dist,\n",
    "              latent_dim=latent_dim,\n",
    "              hidden_dim=hidden_dim,\n",
    "              num_epochs=num_epochs,\n",
    "              batch_size=batch_size,\n",
    "              learning_rate=learning_rate,\n",
    "              learning_rate_decay=learning_rate_decay,\n",
    "              num_layers=num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T21:18:39.930593Z",
     "start_time": "2021-02-01T21:16:35.037013Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# fit\n",
    "losses = cevae.fit(X=torch.tensor(X_train, dtype=torch.float),\n",
    "                   treatment=torch.tensor(treatment_train, dtype=torch.float),\n",
    "                   y=torch.tensor(y_train, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T23:41:35.070742Z",
     "start_time": "2021-02-01T21:18:39.932087Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# predict\n",
    "ite_train = cevae.predict(X_train)\n",
    "ite_val = cevae.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T23:41:35.076150Z",
     "start_time": "2021-02-01T23:41:35.073086Z"
    }
   },
   "outputs": [],
   "source": [
    "ate_train = ite_train.mean()\n",
    "ate_val = ite_val.mean()\n",
    "print(ate_train, ate_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta Learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T23:43:21.827553Z",
     "start_time": "2021-02-01T23:41:35.077523Z"
    }
   },
   "outputs": [],
   "source": [
    "# fit propensity model\n",
    "p_model = ElasticNetPropensityModel()\n",
    "p_train = p_model.fit_predict(X_train, treatment_train)\n",
    "p_val = p_model.fit_predict(X_val, treatment_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T23:43:57.203494Z",
     "start_time": "2021-02-01T23:43:21.829195Z"
    }
   },
   "outputs": [],
   "source": [
    "s_learner = BaseSRegressor(LGBMRegressor())\n",
    "s_ate = s_learner.estimate_ate(X_train, treatment_train, y_train)[0]\n",
    "s_ite_train = s_learner.fit_predict(X_train, treatment_train, y_train)\n",
    "s_ite_val = s_learner.predict(X_val)\n",
    "\n",
    "t_learner = BaseTRegressor(LGBMRegressor())\n",
    "t_ate = t_learner.estimate_ate(X_train, treatment_train, y_train)[0][0]\n",
    "t_ite_train = t_learner.fit_predict(X_train, treatment_train, y_train)\n",
    "t_ite_val = t_learner.predict(X_val, treatment_val, y_val)\n",
    "\n",
    "x_learner = BaseXRegressor(LGBMRegressor())\n",
    "x_ate = x_learner.estimate_ate(X_train, treatment_train, y_train, p_train)[0][0]\n",
    "x_ite_train = x_learner.fit_predict(X_train, treatment_train, y_train, p_train)\n",
    "x_ite_val = x_learner.predict(X_val, treatment_val, y_val, p_val)\n",
    "\n",
    "r_learner = BaseRRegressor(LGBMRegressor())\n",
    "r_ate = r_learner.estimate_ate(X_train, treatment_train, y_train, p_train)[0][0]\n",
    "r_ite_train = r_learner.fit_predict(X_train, treatment_train, y_train, p_train)\n",
    "r_ite_val = r_learner.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Results Comparsion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T23:44:17.848932Z",
     "start_time": "2021-02-01T23:43:57.205878Z"
    }
   },
   "outputs": [],
   "source": [
    "df_preds_train = pd.DataFrame([s_ite_train.ravel(),\n",
    "                               t_ite_train.ravel(),\n",
    "                               x_ite_train.ravel(),\n",
    "                               r_ite_train.ravel(),\n",
    "                               ite_train.ravel(),\n",
    "                               tau_train.ravel(),\n",
    "                               treatment_train.ravel(),\n",
    "                               y_train.ravel()],\n",
    "                               index=['S','T','X','R','CEVAE','tau','w','y']).T\n",
    "\n",
    "df_cumgain_train = get_cumgain(df_preds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T23:44:20.252428Z",
     "start_time": "2021-02-01T23:44:17.850639Z"
    }
   },
   "outputs": [],
   "source": [
    "df_result_train = pd.DataFrame([s_ate, t_ate, x_ate, r_ate, ate_train, tau_train.mean()],\n",
    "                               index=['S','T','X','R','CEVAE','actual'], columns=['ATE'])\n",
    "df_result_train['MAE'] = [mean_absolute_error(t,p) for t,p in zip([s_ite_train, t_ite_train, x_ite_train, r_ite_train, ite_train],\n",
    "                                                                  [tau_train.values.reshape(-1,1)]*5 )\n",
    "                          ] + [None]\n",
    "df_result_train['AUUC'] = auuc_score(df_preds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T23:44:20.261314Z",
     "start_time": "2021-02-01T23:44:20.253755Z"
    }
   },
   "outputs": [],
   "source": [
    "df_result_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T23:44:22.760162Z",
     "start_time": "2021-02-01T23:44:20.262955Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_gain(df_preds_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T23:44:27.616283Z",
     "start_time": "2021-02-01T23:44:22.761877Z"
    }
   },
   "outputs": [],
   "source": [
    "df_preds_val = pd.DataFrame([s_ite_val.ravel(),\n",
    "                             t_ite_val.ravel(),\n",
    "                             x_ite_val.ravel(),\n",
    "                             r_ite_val.ravel(),\n",
    "                             ite_val.ravel(),\n",
    "                             tau_val.ravel(),\n",
    "                             treatment_val.ravel(),\n",
    "                             y_val.ravel()],\n",
    "                             index=['S','T','X','R','CEVAE','tau','w','y']).T\n",
    "\n",
    "df_cumgain_val = get_cumgain(df_preds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T23:44:28.162127Z",
     "start_time": "2021-02-01T23:44:27.617962Z"
    }
   },
   "outputs": [],
   "source": [
    "df_result_val = pd.DataFrame([s_ite_val.mean(), t_ite_val.mean(), x_ite_val.mean(), r_ite_val.mean(), ate_val, tau_val.mean()],\n",
    "                              index=['S','T','X','R','CEVAE','actual'], columns=['ATE'])\n",
    "df_result_val['MAE'] = [mean_absolute_error(t,p) for t,p in zip([s_ite_val, t_ite_val, x_ite_val, r_ite_val, ite_val],\n",
    "                                                                  [tau_val.values.reshape(-1,1)]*5 )\n",
    "                          ] + [None]\n",
    "df_result_val['AUUC'] = auuc_score(df_preds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T23:44:28.169322Z",
     "start_time": "2021-02-01T23:44:28.163676Z"
    }
   },
   "outputs": [],
   "source": [
    "df_result_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T23:44:28.889771Z",
     "start_time": "2021-02-01T23:44:28.170875Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_gain(df_preds_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T23:48:04.322003Z",
     "start_time": "2021-02-01T23:46:46.214260Z"
    }
   },
   "outputs": [],
   "source": [
    "y, X, w, tau, b, e = simulate_hidden_confounder(n=100000, p=5, sigma=1.0, adj=0.)\n",
    "\n",
    "X_train, X_val, y_train, y_val, w_train, w_val, tau_train, tau_val, b_train, b_val, e_train, e_val = \\\n",
    "    train_test_split(X, y, w, tau, b, e, test_size=0.2, random_state=123, shuffle=True)\n",
    "\n",
    "preds_dict_train = {}\n",
    "preds_dict_valid = {}\n",
    "\n",
    "preds_dict_train['Actuals'] = tau_train\n",
    "preds_dict_valid['Actuals'] = tau_val\n",
    "\n",
    "preds_dict_train['generated_data'] = {\n",
    "    'y': y_train,\n",
    "    'X': X_train,\n",
    "    'w': w_train,\n",
    "    'tau': tau_train,\n",
    "    'b': b_train,\n",
    "    'e': e_train}\n",
    "preds_dict_valid['generated_data'] = {\n",
    "    'y': y_val,\n",
    "    'X': X_val,\n",
    "    'w': w_val,\n",
    "    'tau': tau_val,\n",
    "    'b': b_val,\n",
    "    'e': e_val}\n",
    "\n",
    "# Predict p_hat because e would not be directly observed in real-life\n",
    "p_model = ElasticNetPropensityModel()\n",
    "p_hat_train = p_model.fit_predict(X_train, w_train)\n",
    "p_hat_val = p_model.fit_predict(X_val, w_val)\n",
    "\n",
    "for base_learner, label_l in zip([BaseSRegressor, BaseTRegressor, BaseXRegressor, BaseRRegressor],\n",
    "                                 ['S', 'T', 'X', 'R']):\n",
    "    for model, label_m in zip([LinearRegression, XGBRegressor], ['LR', 'XGB']):\n",
    "        # RLearner will need to fit on the p_hat\n",
    "        if label_l != 'R':\n",
    "            learner = base_learner(model())\n",
    "            # fit the model on training data only\n",
    "            learner.fit(X=X_train, treatment=w_train, y=y_train)\n",
    "            try:\n",
    "                preds_dict_train['{} Learner ({})'.format(\n",
    "                    label_l, label_m)] = learner.predict(X=X_train, p=p_hat_train).flatten()\n",
    "                preds_dict_valid['{} Learner ({})'.format(\n",
    "                    label_l, label_m)] = learner.predict(X=X_val, p=p_hat_val).flatten()\n",
    "            except TypeError:\n",
    "                preds_dict_train['{} Learner ({})'.format(\n",
    "                    label_l, label_m)] = learner.predict(X=X_train, treatment=w_train, y=y_train).flatten()\n",
    "                preds_dict_valid['{} Learner ({})'.format(\n",
    "                    label_l, label_m)] = learner.predict(X=X_val, treatment=w_val, y=y_val).flatten()\n",
    "        else:\n",
    "            learner = base_learner(model())\n",
    "            learner.fit(X=X_train, p=p_hat_train, treatment=w_train, y=y_train)\n",
    "            preds_dict_train['{} Learner ({})'.format(\n",
    "                label_l, label_m)] = learner.predict(X=X_train).flatten()\n",
    "            preds_dict_valid['{} Learner ({})'.format(\n",
    "                label_l, label_m)] = learner.predict(X=X_val).flatten()\n",
    "\n",
    "# cevae model settings\n",
    "outcome_dist = \"normal\"\n",
    "latent_dim = 20\n",
    "hidden_dim = 200\n",
    "num_epochs = 5\n",
    "batch_size = 1000\n",
    "learning_rate = 1e-3\n",
    "learning_rate_decay = 0.1\n",
    "num_layers = 3\n",
    "num_samples = 10\n",
    "\n",
    "cevae = CEVAE(outcome_dist=outcome_dist,\n",
    "              latent_dim=latent_dim,\n",
    "              hidden_dim=hidden_dim,\n",
    "              num_epochs=num_epochs,\n",
    "              batch_size=batch_size,\n",
    "              learning_rate=learning_rate,\n",
    "              learning_rate_decay=learning_rate_decay,\n",
    "              num_layers=num_layers,\n",
    "              num_samples=num_samples)\n",
    "\n",
    "# fit\n",
    "losses = cevae.fit(X=torch.tensor(X_train, dtype=torch.float),\n",
    "                   treatment=torch.tensor(w_train, dtype=torch.float),\n",
    "                   y=torch.tensor(y_train, dtype=torch.float))\n",
    "\n",
    "preds_dict_train['CEVAE'] = cevae.predict(X_train).flatten()\n",
    "preds_dict_valid['CEVAE'] = cevae.predict(X_val).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T23:48:04.460479Z",
     "start_time": "2021-02-01T23:48:04.323693Z"
    }
   },
   "outputs": [],
   "source": [
    "actuals_train = preds_dict_train['Actuals']\n",
    "actuals_validation = preds_dict_valid['Actuals']\n",
    "\n",
    "synthetic_summary_train = pd.DataFrame({label: [preds.mean(), mse(preds, actuals_train)] for label, preds\n",
    "                                        in preds_dict_train.items() if 'generated' not in label.lower()},\n",
    "                                       index=['ATE', 'MSE']).T\n",
    "synthetic_summary_train['Abs % Error of ATE'] = np.abs(\n",
    "    (synthetic_summary_train['ATE']/synthetic_summary_train.loc['Actuals', 'ATE']) - 1)\n",
    "\n",
    "synthetic_summary_validation = pd.DataFrame({label: [preds.mean(), mse(preds, actuals_validation)]\n",
    "                                             for label, preds in preds_dict_valid.items()\n",
    "                                             if 'generated' not in label.lower()},\n",
    "                                            index=['ATE', 'MSE']).T\n",
    "synthetic_summary_validation['Abs % Error of ATE'] = np.abs(\n",
    "    (synthetic_summary_validation['ATE']/synthetic_summary_validation.loc['Actuals', 'ATE']) - 1)\n",
    "\n",
    "# calculate kl divergence for training\n",
    "for label in synthetic_summary_train.index:\n",
    "    stacked_values = np.hstack((preds_dict_train[label], actuals_train))\n",
    "    stacked_low = np.percentile(stacked_values, 0.1)\n",
    "    stacked_high = np.percentile(stacked_values, 99.9)\n",
    "    bins = np.linspace(stacked_low, stacked_high, 100)\n",
    "\n",
    "    distr = np.histogram(preds_dict_train[label], bins=bins)[0]\n",
    "    distr = np.clip(distr/distr.sum(), 0.001, 0.999)\n",
    "    true_distr = np.histogram(actuals_train, bins=bins)[0]\n",
    "    true_distr = np.clip(true_distr/true_distr.sum(), 0.001, 0.999)\n",
    "\n",
    "    kl = entropy(distr, true_distr)\n",
    "    synthetic_summary_train.loc[label, 'KL Divergence'] = kl\n",
    "\n",
    "# calculate kl divergence for validation\n",
    "for label in synthetic_summary_validation.index:\n",
    "    stacked_values = np.hstack((preds_dict_valid[label], actuals_validation))\n",
    "    stacked_low = np.percentile(stacked_values, 0.1)\n",
    "    stacked_high = np.percentile(stacked_values, 99.9)\n",
    "    bins = np.linspace(stacked_low, stacked_high, 100)\n",
    "\n",
    "    distr = np.histogram(preds_dict_valid[label], bins=bins)[0]\n",
    "    distr = np.clip(distr/distr.sum(), 0.001, 0.999)\n",
    "    true_distr = np.histogram(actuals_validation, bins=bins)[0]\n",
    "    true_distr = np.clip(true_distr/true_distr.sum(), 0.001, 0.999)\n",
    "\n",
    "    kl = entropy(distr, true_distr)\n",
    "    synthetic_summary_validation.loc[label, 'KL Divergence'] = kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T23:48:07.625291Z",
     "start_time": "2021-02-01T23:48:04.462870Z"
    }
   },
   "outputs": [],
   "source": [
    "df_preds_train = pd.DataFrame([preds_dict_train['S Learner (LR)'].ravel(),\n",
    "                               preds_dict_train['S Learner (XGB)'].ravel(),\n",
    "                               preds_dict_train['T Learner (LR)'].ravel(),\n",
    "                               preds_dict_train['T Learner (XGB)'].ravel(),\n",
    "                               preds_dict_train['X Learner (LR)'].ravel(),\n",
    "                               preds_dict_train['X Learner (XGB)'].ravel(),\n",
    "                               preds_dict_train['R Learner (LR)'].ravel(),\n",
    "                               preds_dict_train['R Learner (XGB)'].ravel(),\n",
    "                               preds_dict_train['CEVAE'].ravel(),\n",
    "                               preds_dict_train['generated_data']['tau'].ravel(),\n",
    "                               preds_dict_train['generated_data']['w'].ravel(),\n",
    "                               preds_dict_train['generated_data']['y'].ravel()],\n",
    "                              index=['S Learner (LR)','S Learner (XGB)',\n",
    "                                     'T Learner (LR)','T Learner (XGB)',\n",
    "                                     'X Learner (LR)','X Learner (XGB)',\n",
    "                                     'R Learner (LR)','R Learner (XGB)',\n",
    "                                     'CEVAE','tau','w','y']).T\n",
    "\n",
    "synthetic_summary_train['AUUC'] = auuc_score(df_preds_train).iloc[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T23:48:08.381588Z",
     "start_time": "2021-02-01T23:48:07.627371Z"
    }
   },
   "outputs": [],
   "source": [
    "df_preds_validation = pd.DataFrame([preds_dict_valid['S Learner (LR)'].ravel(),\n",
    "                               preds_dict_valid['S Learner (XGB)'].ravel(),\n",
    "                               preds_dict_valid['T Learner (LR)'].ravel(),\n",
    "                               preds_dict_valid['T Learner (XGB)'].ravel(),\n",
    "                               preds_dict_valid['X Learner (LR)'].ravel(),\n",
    "                               preds_dict_valid['X Learner (XGB)'].ravel(),\n",
    "                               preds_dict_valid['R Learner (LR)'].ravel(),\n",
    "                               preds_dict_valid['R Learner (XGB)'].ravel(),\n",
    "                               preds_dict_valid['CEVAE'].ravel(),\n",
    "                               preds_dict_valid['generated_data']['tau'].ravel(),\n",
    "                               preds_dict_valid['generated_data']['w'].ravel(),\n",
    "                               preds_dict_valid['generated_data']['y'].ravel()],\n",
    "                              index=['S Learner (LR)','S Learner (XGB)',\n",
    "                                     'T Learner (LR)','T Learner (XGB)',\n",
    "                                     'X Learner (LR)','X Learner (XGB)',\n",
    "                                     'R Learner (LR)','R Learner (XGB)',\n",
    "                                     'CEVAE','tau','w','y']).T\n",
    "\n",
    "synthetic_summary_validation['AUUC'] = auuc_score(df_preds_validation).iloc[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T23:48:08.392392Z",
     "start_time": "2021-02-01T23:48:08.383180Z"
    }
   },
   "outputs": [],
   "source": [
    "synthetic_summary_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T23:48:08.401366Z",
     "start_time": "2021-02-01T23:48:08.393987Z"
    }
   },
   "outputs": [],
   "source": [
    "synthetic_summary_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T23:48:09.079086Z",
     "start_time": "2021-02-01T23:48:08.402848Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_gain(df_preds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T23:48:09.487505Z",
     "start_time": "2021-02-01T23:48:09.083225Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_gain(df_preds_validation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "174px",
    "width": "252px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
